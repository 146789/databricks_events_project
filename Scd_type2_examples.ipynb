{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d703b23-5a76-47a7-b81f-137baa30ed25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_date, to_date, when\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, BooleanType\n",
    "from datetime import date\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SCD_Type2_LeftJoin_Example\").getOrCreate()\n",
    "\n",
    "# Define schema for the existing customer dimension table (target)\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"effective_date\", DateType(), True),\n",
    "    StructField(\"end_date\", DateType(), True),\n",
    "    StructField(\"is_current\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for existing customer dimension table using Python date objects\n",
    "existing_data = [\n",
    "    (1, \"John Doe\", \"123 Old St\", \"john@example.com\", date(2023, 1, 1), None, True),\n",
    "    (2, \"Jane Smith\", \"456 Old Ave\", \"jane@example.com\", date(2023, 1, 1), None, True)\n",
    "]\n",
    "\n",
    "# Create DataFrame for existing customer table\n",
    "customer_df = spark.createDataFrame(existing_data, customer_schema)\n",
    "print(\"customer_df\")\n",
    "customer_df.display()\n",
    "# Define schema for incoming updates (source)\n",
    "update_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"update_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Sample incoming data with updates using Python date objects\n",
    "update_data = [\n",
    "    (1, \"John Doe\", \"789 New St\", \"john.doe@example.com\", date(2023, 6, 1)),  # Address and email changed\n",
    "    (2, \"Jane Smith\", \"456 Old Ave\", \"jane@example.com\", date(2023, 6, 1)),   # No change\n",
    "    (3, \"Bob Wilson\", \"101 New Rd\", \"bob@example.com\", date(2023, 6, 1))      # New customer\n",
    "]\n",
    "\n",
    "# Create DataFrame for incoming updates\n",
    "updates_df = spark.createDataFrame(update_data, update_schema)\n",
    "print(\"updates_df\")\n",
    "updates_df.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd5367e-6b0d-48cc-85e8-2152a82eee32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = updates_df.join(\n",
    "    customer_df,\n",
    "    (updates_df.customer_id == customer_df.customer_id) & (customer_df.is_current == True),\n",
    "    \"left_outer\"\n",
    ")\n",
    "print(\"joined_df\")\n",
    "joined_df.display()\n",
    "\n",
    "filtered_df = joined_df.filter(\n",
    "    # New records (no match in customer_df) or changed records\n",
    "    (customer_df.customer_id.isNull()) |\n",
    "    (updates_df.address != customer_df.address) |\n",
    "    (updates_df.email != customer_df.email)\n",
    ").select(\n",
    "    updates_df.customer_id,\n",
    "    updates_df.name,\n",
    "    updates_df.address,\n",
    "    updates_df.email,\n",
    "    updates_df.update_date.alias(\"effective_date\"),\n",
    "    lit(None).cast(DateType()).alias(\"end_date\"),\n",
    "    lit(True).alias(\"is_current\")\n",
    ")\n",
    "\n",
    "filtered_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4188f711-b1eb-4bea-93f4-d281c1a89a3e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1747067729883}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expired_data = joined_df.filter((customer_df.customer_id.isNotNull()) &\n",
    "    ((updates_df.address != customer_df.address) | (updates_df.email != customer_df.email)))\\\n",
    "    .select(\n",
    "    customer_df.customer_id,\n",
    "    customer_df.name,\n",
    "    customer_df.address,\n",
    "    customer_df.email,\n",
    "    customer_df.effective_date,\n",
    "    updates_df.update_date.alias(\"end_date\"),\n",
    "    lit(False).alias(\"is_current\"))\n",
    "expired_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49350838-3da8-49c7-8c2f-e66a01595f37",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"name\":{\"format\":{\"preset\":\"string-preset-url\"}},\"address\":{\"format\":{\"preset\":\"string-preset-url\"}},\"email\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1747067915167}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unchanged_records = customer_df.join(\n",
    "    filtered_df,\n",
    "    customer_df.customer_id == updates_df.customer_id,\n",
    "    \"left_anti\"\n",
    ").select(\n",
    "    customer_df.customer_id,\n",
    "    customer_df.name,\n",
    "    customer_df.address,\n",
    "    customer_df.email,\n",
    "    customer_df.effective_date,\n",
    "    customer_df.end_date,\n",
    "    customer_df.is_current\n",
    ")\n",
    "unchanged_records.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "972a8556-6c03-403b-9953-bde46684b82d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = filtered_df.union(expired_data).union(unchanged_records)\n",
    "final_df.display()\n",
    "final_df.orderBy(\"customer_id\", \"effective_date\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "264f69da-d372-4fdb-9be6-b9e583ca1972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Source DataFrame (latest snapshot from source system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bee3086-543d-409f-9b6e-ea9013f575ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_source_data = [\n",
    "    (101, \"Alice Smith\", \"Chicago\"),  # City changed\n",
    "    (102, \"Bob Brown\", \"Seattle\")     # No change\n",
    "]\n",
    "df_new_source = spark.createDataFrame(new_source_data, [\"CustomerID\", \"Name\", \"City\"])\n",
    "df_new_source.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e06f46b6-5805-4ab0-bd95-f7f0ad0b9a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Target Dimension DataFrame (Customer_Dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8d001c-9e54-401d-99a3-6e077e0d9688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, lit\n",
    "\n",
    "# Initial load - simulate source\n",
    "initial_source_data = [\n",
    "    (101, \"Alice Smith\", \"Boston\"),\n",
    "    (102, \"Bob Brown\", \"Seattle\")\n",
    "]\n",
    "\n",
    "df_initial_source = spark.createDataFrame(initial_source_data, [\"CustomerID\", \"Name\", \"City\"])\n",
    "\n",
    "# Add SCD Type 2 columns\n",
    "df_initial_target = df_initial_source\\\n",
    "    .withColumn(\"StartDate\", current_date()) \\\n",
    "    .withColumn(\"EndDate\", lit(\"9999-12-31\").cast(\"date\")) \\\n",
    "    .withColumn(\"IsCurrent\", lit(True))\n",
    "\n",
    "# Reorder columns\n",
    "df_initial_target = df_initial_target.select(\n",
    "    \"CustomerID\", \"Name\", \"City\", \"StartDate\", \"EndDate\", \"IsCurrent\"\n",
    ")\n",
    "\n",
    "df_initial_target.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3124087f-b295-4cdd-90b6-3d6b3f825e4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = df_new_source.join(df_initial_target, (df_new_source.CustomerID == df_initial_target.CustomerID) & (df_initial_target.IsCurrent == True), \"left_outer\")\n",
    "joined_df.display()\n",
    "\n",
    "changed_df = joined_df.filter((df_initial_target.CustomerID.isNull()) | (df_new_source.City != df_initial_target.City) | (df_new_source.Name != df_initial_target.Name)).select(\n",
    "    df_new_source.CustomerID,\n",
    "    df_new_source.Name,\n",
    "    df_new_source.City,\n",
    "    date_add(current_date(),1).alias(\"StartDate\"),\n",
    "    lit(\"9999-12-31\").cast(\"date\").alias(\"EndDate\"),\n",
    "    lit(True).alias(\"IsCurrent\")\n",
    ")\n",
    "changed_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdcac645-f9ae-4751-b981-8ee5f7dbd6f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, lit, date_sub, date_add\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea34737-d658-40f3-9229-2b54a5292d23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expired_df = joined_df.filter((df_initial_target.CustomerID.isNotNull()) & (df_new_source.City != df_initial_target.City) | (df_new_source.Name != df_initial_target.Name))\n",
    "expired_df.display()\n",
    "\n",
    "expired_df = expired_df.select(\n",
    "    df_initial_target.CustomerID,\n",
    "    df_initial_target.Name,\n",
    "    df_initial_target.City,\n",
    "    df_initial_target.StartDate,\n",
    "    current_date().alias(\"EndDate\"),\n",
    "    lit(False).alias(\"IsCurrent\")\n",
    ")\n",
    "expired_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e86e9f6-b937-4e60-af87-b93b5f485075",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1747074275080}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unchanged_records = df_initial_target.join(changed_df, \"CustomerID\", \"left_anti\")\n",
    "unchanged_records.display()\n",
    "\n",
    "final_df = unchanged_records.unionByName(changed_df).unionByName(expired_df)\n",
    "final_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cceceecd-b6d0-42ed-843e-0701e6a2e1ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, BooleanType\n",
    "from datetime import date\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Initial dimension data with historical StartDate\n",
    "initial_data = [\n",
    "    (1, 1001, \"iPhone 14\", \"Mobile\", 999.0, date(2024, 1, 1), date(9999, 12, 31), True),\n",
    "    (2, 1002, \"Galaxy S23\", \"Mobile\", 849.0, date(2024, 1, 1), date(9999, 12, 31), True),\n",
    "    (3, 1003, \"Dell XPS 13\", \"Laptop\", 1199.0, date(2024, 1, 1), date(9999, 12, 31), True)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"SurrogateKey\", IntegerType(), False),\n",
    "    StructField(\"ProductID\", IntegerType(), False),\n",
    "    StructField(\"ProductName\", StringType(), False),\n",
    "    StructField(\"Category\", StringType(), False),\n",
    "    StructField(\"Price\", DoubleType(), False),\n",
    "    StructField(\"StartDate\", DateType(), False),\n",
    "    StructField(\"EndDate\", DateType(), False),\n",
    "    StructField(\"IsCurrent\", BooleanType(), False)\n",
    "])\n",
    "\n",
    "df_dim_initial = spark.createDataFrame(initial_data, schema)\n",
    "df_dim_initial.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1d9c01d-7cc2-431a-801b-97311fb0f825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# New snapshot from source (today's data)\n",
    "new_products = [\n",
    "    (1001, \"iPhone 14\", \"Mobile\", 999.0),       # No change\n",
    "    (1002, \"Galaxy S23\", \"Mobile\", 899.0),      # Price updated\n",
    "    (1003, \"Dell XPS 13\", \"Laptop\", 1199.0)     # No change\n",
    "]\n",
    "\n",
    "df_source = spark.createDataFrame(new_products, [\"ProductID\", \"ProductName\", \"Category\", \"Price\"])\n",
    "df_source.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594d5509-1190-4241-9a10-edfe061aaef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_condition = (\n",
    "    (df_source.ProductID  ==  df_dim_initial.ProductID) & (df_dim_initial.IsCurrent == True)\n",
    ")\n",
    "joined_df  = df_source.join(df_dim_initial, join_condition\n",
    "                            , \"left_outer\")\n",
    "joined_df.display()\n",
    "\n",
    "changed_df = joined_df.filter((df_dim_initial.ProductID.isNull()) | (df_source.Price != df_dim_initial.Price))\n",
    "changed_df.display()\n",
    "\n",
    "changed_df = changed_df.select(\n",
    "    df_source.ProductID.alias(\"ProductID\"),\n",
    "    df_source.ProductName.alias(\"ProductName\"),\n",
    "    df_source.Category.alias(\"Category\"),\n",
    "    df_source.Price.alias(\"Price\"),\n",
    "    date_add(current_date(),1).alias(\"StartDate\"),\n",
    "    lit(\"9999-12-31\").cast(\"date\").alias(\"EndDate\"),\n",
    "    lit(True).alias(\"IsCurrent\")\n",
    ")\n",
    "changed_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8474903-892d-4c2f-a626-a821f43b5e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expired_df = joined_df.filter((df_dim_initial.ProductID.isNotNull()) & (df_source.Price != df_dim_initial.Price))\n",
    "expired_df.display()\n",
    "expired_df = expired_df.select(\n",
    "    df_dim_initial.ProductID.alias(\"ProductID\"),\n",
    "    df_dim_initial.ProductName.alias(\"ProductName\"),\n",
    "    df_dim_initial.Category.alias(\"Category\"),\n",
    "    df_dim_initial.Price.alias(\"Price\"),\n",
    "    df_dim_initial.StartDate.alias(\"StartDate\"),\n",
    "    current_date().alias(\"EndDate\"),\n",
    "    lit(False).alias(\"IsCurrent\")\n",
    ")\n",
    "expired_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b686d5-bb83-4e76-ac87-57ab89e6f70f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unchanged_records = df_dim_initial.join(changed_df, \"ProductID\", \"left_anti\").drop(\"SurrogateKey\")\n",
    "unchanged_records.display()\n",
    "final_df = unchanged_records.unionByName(changed_df).unionByName(expired_df)\n",
    "final_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae936107-1716-468f-b290-e6f43fad9600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Scd_type2_examples",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
