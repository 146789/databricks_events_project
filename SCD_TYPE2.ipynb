{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a1960c-49d9-4326-8730-748674b3efbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp, sum as _sum\n",
    "from delta.tables import DeltaTable\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationResult, VerificationSuite\n",
    "import os\n",
    "\n",
    "print(os.environ['SPARK_VERSION'])\n",
    "date_str = \"2024-07-25\"\n",
    "\n",
    "booking_data = f\"/Volumes/incremental_load/project1/orders_data/booking_data/bookings_{date_str}.csv\"\n",
    "customer_data = f\"/Volumes/incremental_load/project1/orders_data/customer_data/customers_{date_str}.csv\"\n",
    "print(booking_data)\n",
    "print(customer_data)\n",
    "\n",
    "\n",
    "booking_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"quote\", \"\\\"\")\n",
    "            .option(\"multiline\", \"true\")\n",
    "            .load(booking_data)\n",
    ")\n",
    "\n",
    "booking_df.printSchema()\n",
    "display(booking_df)\n",
    "\n",
    "customer_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"quote\", \"\\\"\")\n",
    "            .option(\"multiline\", \"true\")\n",
    "            .load(customer_data)\n",
    ")\n",
    "\n",
    "customer_df.printSchema()\n",
    "display(customer_df)\n",
    "\n",
    "check_incremental = Check(spark, CheckLevel.Error, \"Bookings Data check\") \\\n",
    "                    .hasSize(lambda x: x > 0) \\\n",
    "                    .isComplete(\"booking_id\", hint=\"Booking ID is not unique throught\")\\\n",
    "                    .isComplete(\"amount\")\\\n",
    "                    .isNonNegative(\"amount\")\\\n",
    "                    .isNonNegative(\"quantity\")\\\n",
    "                    .isNonNegative(\"discount\")\n",
    "\n",
    "check_scd = Check(spark, CheckLevel.Error, \"Customer Data check\") \\\n",
    "                    .hasSize(lambda x: x > 0)\\\n",
    "                    .isUnique(\"customer_id\")\\\n",
    "                    .isComplete(\"customer_name\")\\\n",
    "                    .isComplete(\"customer_address\")\\\n",
    "                    .isComplete(\"email\")\n",
    "booking_dq_check = VerificationSuite(spark)\\\n",
    "                .onData(booking_df)\\\n",
    "                .addCheck(check_incremental)\\\n",
    "                .run()\n",
    "\n",
    "customer_dq_check = VerificationSuite(spark)\\\n",
    "                .onData(customer_df)\\\n",
    "                .addCheck(check_scd)\\\n",
    "                .run()\n",
    "booking_dq_check_df = VerificationResult.checkResultsAsDataFrame(spark, booking_dq_check)\n",
    "display(booking_dq_check_df)\n",
    "customer_dq_check_df = VerificationResult.checkResultsAsDataFrame(spark, customer_dq_check)\n",
    "display(customer_dq_check_df)\n",
    "\n",
    "if booking_dq_check.status != \"Success\":\n",
    "    raise ValueError(\"Data quality check failed for booking data\")\n",
    "\n",
    "if customer_dq_check.status != \"Success\":\n",
    "    raise ValueError(\"Data quality check failed for customer data\")\n",
    "\n",
    "booking_df_incremental = booking_df.withColumn(\"ingestion_time\", current_timestamp())\n",
    "\n",
    "df_joined = booking_df_incremental.join(customer_df, [\"customer_id\"], \"inner\")\n",
    "\n",
    "df_transformed = df_joined\\\n",
    "                .withColumn(\"total_cost\", col(\"amount\") - col(\"discount\"))\\\n",
    "                .filter(col(\"quantity\") > 0)\n",
    "df_transformed_agg = df_transformed \\\n",
    "                .groupBy(\"booking_type\", \"customer_id\") \\\n",
    "                .agg(_sum(\"total_cost\").alias(\"total_amount_sum\"),\n",
    "                    _sum(\"quantity\").alias(\"total_quantity_sum\")\n",
    "                )\n",
    "fact_table_path = \"incremental_load.default.booking_fact\"\n",
    "fact_table_exists = spark.catalog.tableExists(fact_table_path)\n",
    "\n",
    "if fact_table_exists:\n",
    "    df_existting_fact = spark.read.format(\"delta\").table(fact_table_path)\n",
    "\n",
    "    df_combined = df_existting_fact.unionByName(df_transformed_agg, allowMissingColumns=True)\n",
    "\n",
    "    df_final_agg = df_combined \\\n",
    "                    .groupBy(\"booking_type\") \\\n",
    "                    .agg(_sum(\"total_amount_sum\").alias(\"total_amount_sum\"),                     _sum(\"total_quantity_sum\").alias(\"total_quantity_sum\"))\n",
    "else:\n",
    "    df_final_agg = df_transformed_agg\n",
    "display(df_final_agg)\n",
    "\n",
    "df_final_agg.write.format(\"delta\").modet(\"overwrite\").option(\"overwriteSchema\", \"true\").saveASTable(fact_table_path)\n",
    "\n",
    "\n",
    "scd_table_path = \"incremental_load.default.customer_dim\"\n",
    "scd_table_exists = spark.catalog.tableExists(scd_table_path)\n",
    "\n",
    "if scd_table_exists:\n",
    "    scd_table = DeltaTable.forName(spark, scd_table_path)\n",
    "    display(scd_table.toDF())\n",
    "\n",
    "    scd_table.alias(\"scd\")\\\n",
    "        .merge(\n",
    "            customer_df.alias(\"updates\"),\n",
    "            \"scd.customer_id = updates.customer_id and scd.valid_to = '9999-12-31'\"\n",
    "        ).whenMatchedUpdate(\n",
    "            set = {\n",
    "                \"valid_to\": \"updates.from\"\n",
    "            }\n",
    "        )\\\n",
    "        .execute()\n",
    "    customer_df.write.format(\"delta\").mode(\"append\").saveAsTable(scd_table_path)\n",
    "else:\n",
    "    customer_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(scd_table_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea609bec-60e1-41ef-91f1-a3fd8604b157",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ExplodeExample\").getOrCreate()\n",
    "\n",
    "# Sample data as a list of dictionaries (could be loaded from a JSON file)\n",
    "data = [\n",
    "    {\"name\": \"Alice\", \"age\": 30, \"experience\": [{\"company\": \"Company A\", \"years\": 2}]},\n",
    "    {\"name\": \"Bob\", \"age\": 35, \"experience\": [{\"company\": \"Company B\", \"years\": 3},\n",
    "                                              {\"company\": \"Company C\", \"years\": 4}]}\n",
    "]\n",
    "\n",
    "# Define the schema for the data\n",
    "schema = \"name STRING, age INT, experience ARRAY<STRUCT<company: STRING, years: INT>>\"\n",
    "\n",
    "# Create a DataFrame from the sample data\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the original DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36087e52-1f80-496b-8373-ca6593c253f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explode the experience array into separate rows\n",
    "df_exploded = df.select(\"name\", explode(\"experience\").alias(\"exp\"))\n",
    "\n",
    "# Flatten the struct inside the exploded column\n",
    "df_flat = df_exploded.select(\n",
    "    \"name\",\n",
    "    \"exp.company\",\n",
    "    \"exp.years\"\n",
    ")\n",
    "\n",
    "# Show the exploded DataFrame\n",
    "df_flat.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70a3baa8-1446-41d3-a8d2-3ac54a5990d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"ExplodeMapExample\").getOrCreate()\n",
    "\n",
    "# Sample data with map (dictionary)\n",
    "data = [\n",
    "    {\"name\": \"Alice\", \"age\": 30, \"skills\": {\"Python\": 5, \"Spark\": 4, \"SQL\": 3}},\n",
    "    {\"name\": \"Bob\", \"age\": 35, \"skills\": {\"Java\": 5, \"Scala\": 4}}\n",
    "]\n",
    "\n",
    "# Define the schema\n",
    "schema = \"name STRING, age INT, skills MAP<STRING, INT>\"\n",
    "\n",
    "# Create a DataFrame from the sample data\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the original DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69898ff5-2cf0-4fd9-bd7a-ea33b41d038a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explode the skills map into individual rows (key-value pairs)\n",
    "df_exploded = df.select(\"name\", \"age\", explode(\"skills\").alias(\"skill_name\", \"skill_rating\"))\n",
    "\n",
    "# Show the exploded DataFrame\n",
    "df_exploded.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD_TYPE2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
