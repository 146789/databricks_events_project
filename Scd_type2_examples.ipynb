{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d703b23-5a76-47a7-b81f-137baa30ed25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_date, to_date, when\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, BooleanType\n",
    "from datetime import date\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SCD_Type2_LeftJoin_Example\").getOrCreate()\n",
    "\n",
    "# Define schema for the existing customer dimension table (target)\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"effective_date\", DateType(), True),\n",
    "    StructField(\"end_date\", DateType(), True),\n",
    "    StructField(\"is_current\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for existing customer dimension table using Python date objects\n",
    "existing_data = [\n",
    "    (1, \"John Doe\", \"123 Old St\", \"john@example.com\", date(2023, 1, 1), None, True),\n",
    "    (2, \"Jane Smith\", \"456 Old Ave\", \"jane@example.com\", date(2023, 1, 1), None, True)\n",
    "]\n",
    "\n",
    "# Create DataFrame for existing customer table\n",
    "customer_df = spark.createDataFrame(existing_data, customer_schema)\n",
    "print(\"customer_df\")\n",
    "customer_df.display()\n",
    "# Define schema for incoming updates (source)\n",
    "update_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"update_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Sample incoming data with updates using Python date objects\n",
    "update_data = [\n",
    "    (1, \"John Doe\", \"789 New St\", \"john.doe@example.com\", date(2023, 6, 1)),  # Address and email changed\n",
    "    (2, \"Jane Smith\", \"456 Old Ave\", \"jane@example.com\", date(2023, 6, 1)),   # No change\n",
    "    (3, \"Bob Wilson\", \"101 New Rd\", \"bob@example.com\", date(2023, 6, 1))      # New customer\n",
    "]\n",
    "\n",
    "# Create DataFrame for incoming updates\n",
    "updates_df = spark.createDataFrame(update_data, update_schema)\n",
    "print(\"updates_df\")\n",
    "updates_df.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd5367e-6b0d-48cc-85e8-2152a82eee32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = updates_df.join(\n",
    "    customer_df,\n",
    "    (updates_df.customer_id == customer_df.customer_id) & (customer_df.is_current == True),\n",
    "    \"left_outer\"\n",
    ")\n",
    "print(\"joined_df\")\n",
    "joined_df.display()\n",
    "\n",
    "filtered_df = joined_df.filter(\n",
    "    # New records (no match in customer_df) or changed records\n",
    "    (customer_df.customer_id.isNull()) |\n",
    "    (updates_df.address != customer_df.address) |\n",
    "    (updates_df.email != customer_df.email)\n",
    ").select(\n",
    "    updates_df.customer_id,\n",
    "    updates_df.name,\n",
    "    updates_df.address,\n",
    "    updates_df.email,\n",
    "    updates_df.update_date.alias(\"effective_date\"),\n",
    "    lit(None).cast(DateType()).alias(\"end_date\"),\n",
    "    lit(True).alias(\"is_current\")\n",
    ")\n",
    "\n",
    "filtered_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4188f711-b1eb-4bea-93f4-d281c1a89a3e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1747067729883}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expired_data = joined_df.filter((customer_df.customer_id.isNotNull()) &\n",
    "    ((updates_df.address != customer_df.address) | (updates_df.email != customer_df.email)))\\\n",
    "    .select(\n",
    "    customer_df.customer_id,\n",
    "    customer_df.name,\n",
    "    customer_df.address,\n",
    "    customer_df.email,\n",
    "    customer_df.effective_date,\n",
    "    updates_df.update_date.alias(\"end_date\"),\n",
    "    lit(False).alias(\"is_current\"))\n",
    "expired_data.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49350838-3da8-49c7-8c2f-e66a01595f37",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"name\":{\"format\":{\"preset\":\"string-preset-url\"}},\"address\":{\"format\":{\"preset\":\"string-preset-url\"}},\"email\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1747067915167}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unchanged_records = customer_df.join(\n",
    "    filtered_df,\n",
    "    customer_df.customer_id == updates_df.customer_id,\n",
    "    \"left_anti\"\n",
    ").select(\n",
    "    customer_df.customer_id,\n",
    "    customer_df.name,\n",
    "    customer_df.address,\n",
    "    customer_df.email,\n",
    "    customer_df.effective_date,\n",
    "    customer_df.end_date,\n",
    "    customer_df.is_current\n",
    ")\n",
    "unchanged_records.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "972a8556-6c03-403b-9953-bde46684b82d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = filtered_df.union(expired_data).union(unchanged_records)\n",
    "final_df.display()\n",
    "final_df.orderBy(\"customer_id\", \"effective_date\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "264f69da-d372-4fdb-9be6-b9e583ca1972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Source DataFrame (latest snapshot from source system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bee3086-543d-409f-9b6e-ea9013f575ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_source_data = [\n",
    "    (101, \"Alice Smith\", \"Chicago\"),  # City changed\n",
    "    (102, \"Bob Brown\", \"Seattle\")     # No change\n",
    "]\n",
    "df_new_source = spark.createDataFrame(new_source_data, [\"CustomerID\", \"Name\", \"City\"])\n",
    "df_new_source.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e06f46b6-5805-4ab0-bd95-f7f0ad0b9a7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Target Dimension DataFrame (Customer_Dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8d001c-9e54-401d-99a3-6e077e0d9688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, lit\n",
    "\n",
    "# Initial load - simulate source\n",
    "initial_source_data = [\n",
    "    (101, \"Alice Smith\", \"Boston\"),\n",
    "    (102, \"Bob Brown\", \"Seattle\")\n",
    "]\n",
    "\n",
    "df_initial_source = spark.createDataFrame(initial_source_data, [\"CustomerID\", \"Name\", \"City\"])\n",
    "\n",
    "# Add SCD Type 2 columns\n",
    "df_initial_target = df_initial_source\\\n",
    "    .withColumn(\"StartDate\", current_date()) \\\n",
    "    .withColumn(\"EndDate\", lit(\"9999-12-31\").cast(\"date\")) \\\n",
    "    .withColumn(\"IsCurrent\", lit(True))\n",
    "\n",
    "# Reorder columns\n",
    "df_initial_target = df_initial_target.select(\n",
    "    \"CustomerID\", \"Name\", \"City\", \"StartDate\", \"EndDate\", \"IsCurrent\"\n",
    ")\n",
    "\n",
    "df_initial_target.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3124087f-b295-4cdd-90b6-3d6b3f825e4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = df_new_source.join(df_initial_target, (df_new_source.CustomerID == df_initial_target.CustomerID) & (df_initial_target.IsCurrent == True), \"left_outer\")\n",
    "joined_df.display()\n",
    "\n",
    "changed_df = joined_df.filter((df_initial_target.CustomerID.isNull()) | (df_new_source.City != df_initial_target.City) | (df_new_source.Name != df_initial_target.Name)).select(\n",
    "    df_new_source.CustomerID,\n",
    "    df_new_source.Name,\n",
    "    df_new_source.City,\n",
    "    date_add(current_date(),1).alias(\"StartDate\"),\n",
    "    lit(\"9999-12-31\").cast(\"date\").alias(\"EndDate\"),\n",
    "    lit(True).alias(\"IsCurrent\")\n",
    ")\n",
    "changed_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdcac645-f9ae-4751-b981-8ee5f7dbd6f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, lit, date_sub, date_add\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea34737-d658-40f3-9229-2b54a5292d23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expired_df = joined_df.filter((df_initial_target.CustomerID.isNotNull()) & (df_new_source.City != df_initial_target.City) | (df_new_source.Name != df_initial_target.Name))\n",
    "expired_df.display()\n",
    "\n",
    "expired_df = expired_df.select(\n",
    "    df_initial_target.CustomerID,\n",
    "    df_initial_target.Name,\n",
    "    df_initial_target.City,\n",
    "    df_initial_target.StartDate,\n",
    "    current_date().alias(\"EndDate\"),\n",
    "    lit(False).alias(\"IsCurrent\")\n",
    ")\n",
    "expired_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e86e9f6-b937-4e60-af87-b93b5f485075",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1747074275080}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unchanged_records = df_initial_target.join(changed_df, \"CustomerID\", \"left_anti\")\n",
    "unchanged_records.display()\n",
    "\n",
    "final_df = unchanged_records.unionByName(changed_df).unionByName(expired_df)\n",
    "final_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cceceecd-b6d0-42ed-843e-0701e6a2e1ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, BooleanType\n",
    "from datetime import date\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Initial dimension data with historical StartDate\n",
    "initial_data = [\n",
    "    (1, 1001, \"iPhone 14\", \"Mobile\", 999.0, date(2024, 1, 1), date(9999, 12, 31), True),\n",
    "    (2, 1002, \"Galaxy S23\", \"Mobile\", 849.0, date(2024, 1, 1), date(9999, 12, 31), True),\n",
    "    (3, 1003, \"Dell XPS 13\", \"Laptop\", 1199.0, date(2024, 1, 1), date(9999, 12, 31), True)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"SurrogateKey\", IntegerType(), False),\n",
    "    StructField(\"ProductID\", IntegerType(), False),\n",
    "    StructField(\"ProductName\", StringType(), False),\n",
    "    StructField(\"Category\", StringType(), False),\n",
    "    StructField(\"Price\", DoubleType(), False),\n",
    "    StructField(\"StartDate\", DateType(), False),\n",
    "    StructField(\"EndDate\", DateType(), False),\n",
    "    StructField(\"IsCurrent\", BooleanType(), False)\n",
    "])\n",
    "\n",
    "df_dim_initial = spark.createDataFrame(initial_data, schema)\n",
    "df_dim_initial.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1d9c01d-7cc2-431a-801b-97311fb0f825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# New snapshot from source (today's data)\n",
    "new_products = [\n",
    "    (1001, \"iPhone 14\", \"Mobile\", 999.0),       # No change\n",
    "    (1002, \"Galaxy S23\", \"Mobile\", 899.0),      # Price updated\n",
    "    (1003, \"Dell XPS 13\", \"Laptop\", 1199.0)     # No change\n",
    "]\n",
    "\n",
    "df_source = spark.createDataFrame(new_products, [\"ProductID\", \"ProductName\", \"Category\", \"Price\"])\n",
    "df_source.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594d5509-1190-4241-9a10-edfe061aaef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "join_condition = (\n",
    "    (df_source.ProductID  ==  df_dim_initial.ProductID) & (df_dim_initial.IsCurrent == True)\n",
    ")\n",
    "joined_df  = df_source.join(df_dim_initial, join_condition\n",
    "                            , \"left_outer\")\n",
    "joined_df.display()\n",
    "\n",
    "changed_df = joined_df.filter((df_dim_initial.ProductID.isNull()) | (df_source.Price != df_dim_initial.Price))\n",
    "changed_df.display()\n",
    "\n",
    "changed_df = changed_df.select(\n",
    "    df_source.ProductID.alias(\"ProductID\"),\n",
    "    df_source.ProductName.alias(\"ProductName\"),\n",
    "    df_source.Category.alias(\"Category\"),\n",
    "    df_source.Price.alias(\"Price\"),\n",
    "    date_add(current_date(),1).alias(\"StartDate\"),\n",
    "    lit(\"9999-12-31\").cast(\"date\").alias(\"EndDate\"),\n",
    "    lit(True).alias(\"IsCurrent\")\n",
    ")\n",
    "changed_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8474903-892d-4c2f-a626-a821f43b5e9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expired_df = joined_df.filter((df_dim_initial.ProductID.isNotNull()) & (df_source.Price != df_dim_initial.Price))\n",
    "expired_df.display()\n",
    "expired_df = expired_df.select(\n",
    "    df_dim_initial.ProductID.alias(\"ProductID\"),\n",
    "    df_dim_initial.ProductName.alias(\"ProductName\"),\n",
    "    df_dim_initial.Category.alias(\"Category\"),\n",
    "    df_dim_initial.Price.alias(\"Price\"),\n",
    "    df_dim_initial.StartDate.alias(\"StartDate\"),\n",
    "    current_date().alias(\"EndDate\"),\n",
    "    lit(False).alias(\"IsCurrent\")\n",
    ")\n",
    "expired_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b686d5-bb83-4e76-ac87-57ab89e6f70f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unchanged_records = df_dim_initial.join(changed_df, \"ProductID\", \"left_anti\").drop(\"SurrogateKey\")\n",
    "unchanged_records.display()\n",
    "final_df = unchanged_records.unionByName(changed_df).unionByName(expired_df)\n",
    "final_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae936107-1716-468f-b290-e6f43fad9600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13305734-aa99-4d2b-b422-153ef7b04ce2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "initial"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_date, lit\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType\n",
    "\n",
    "# Create a Spark session (should already exist in Databricks)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Initial customer data (Day 1)\n",
    "initial_data = [\n",
    "    (1, \"Alice\", \"New York\"),\n",
    "    (2, \"Bob\", \"San Francisco\"),\n",
    "    (3, \"Charlie\", \"Chicago\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_initial = spark.createDataFrame(initial_data, schema)\n",
    "\n",
    "# Add SCD2 columns\n",
    "df_initial = df_initial.withColumn(\"start_date\", current_date()) \\\n",
    "    .withColumn(\"end_date\", lit(\"9999-12-31\")) \\\n",
    "    .withColumn(\"is_current\", lit(True))\n",
    "df_initial.display()\n",
    "\n",
    "# Save as Delta table\n",
    "df_initial.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"incremental_load.default.dim_customers_scd2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2e6da4f-ae39-4cd5-b78b-4ef88a1c9669",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "day2"
    }
   },
   "outputs": [],
   "source": [
    "# Day 2 source data with changes\n",
    "day2_data = [\n",
    "    (1, \"Alice\", \"New York\"),           # unchanged\n",
    "    (2, \"Bob\", \"Los Angeles\"),          # changed city\n",
    "    (4, \"David\", \"Houston\")             # new customer\n",
    "]\n",
    "\n",
    "df_day2 = spark.createDataFrame(day2_data, schema)\n",
    "\n",
    "# Add SCD2 columns to incoming data\n",
    "df_day2 = df_day2.withColumn(\"start_date\", current_date()) \\\n",
    "    .withColumn(\"end_date\", lit(\"9999-12-31\")) \\\n",
    "    .withColumn(\"is_current\", lit(True))\n",
    "df_day2.display()\n",
    "\n",
    "# Register as temp view for SQL-based MERGE\n",
    "df_day2.createOrReplaceTempView(\"staging_customers\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4d91be3-e417-4cab-90cb-cba5afc1239e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "MERGE INTO incremental_load.default.dim_customers_scd2 AS target\n",
    "USING staging_customers AS source\n",
    "ON target.customer_id = source.customer_id AND target.is_current = true\n",
    "\n",
    "WHEN MATCHED AND target.city <> source.city THEN\n",
    "  UPDATE SET target.end_date = '2024-05-13', target.is_current = false\n",
    "\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (customer_id, name, city, start_date, end_date, is_current)\n",
    "  VALUES (source.customer_id, source.name, source.city, source.start_date, source.end_date, source.is_current)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9a0f214-02cb-404b-8b95-7eaa464cc269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM incremental_load.default.dim_customers_scd2 ORDER BY customer_id, start_date\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4034b9c6-fd63-402e-942c-2a404bd6e0cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM incremental_load.default.dim_customers_scd2 \n",
    "WHERE customer_id = 2 \n",
    "ORDER BY start_date;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc265419-3e11-411b-a411-67c6aef31871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, current_date, when\n",
    "from datetime import date\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SCD Type 2 with datetime.date\").getOrCreate()\n",
    "\n",
    "# Step 1: Simulated source data (incoming data)\n",
    "source_data = [\n",
    "    (1, \"Alice\", \"USA\"),     # Unchanged\n",
    "    (2, \"Bob\", \"Canada\"),    # Updated (was UK)\n",
    "    (4, \"Daisy\", \"India\"),   # New record\n",
    "]\n",
    "source_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"country\", StringType(), False),\n",
    "])\n",
    "source_df = spark.createDataFrame(source_data, schema=source_schema)\n",
    "\n",
    "# Step 2: Existing dimension table (dim_customer)\n",
    "# Use datetime.date to create valid dates in dim_data\n",
    "dim_data = [\n",
    "    (1, \"Alice\", \"USA\", date(2020, 1, 1), None),  # Full date (year-month-day)\n",
    "    (2, \"Bob\", \"UK\", date(2019, 1, 1), None),     # Full date (year-month-day)\n",
    "    (3, \"Charlie\", \"Germany\", date(2021, 1, 1), None),  # Full date (year-month-day)\n",
    "]\n",
    "dim_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"country\", StringType(), False),\n",
    "    StructField(\"start_date\", DateType(), False),\n",
    "    StructField(\"end_date\", DateType(), True),\n",
    "])\n",
    "\n",
    "# Create the DataFrame using the datetime.date objects in dim_data\n",
    "dim_df = spark.createDataFrame(dim_data, schema=dim_schema)\n",
    "\n",
    "# Step 3: Perform SCD Type 2 Merge - Maintain History\n",
    "\n",
    "# Add today's date for merge logic\n",
    "today = date.today()  # Today's date using datetime.date()\n",
    "\n",
    "# Step 4: Prepare source data to include start_date and end_date columns\n",
    "source_with_dates = source_df.withColumn(\"start_date\", lit(today)).withColumn(\"end_date\", lit(None).cast(\"date\"))\n",
    "\n",
    "# Create temp view for SQL merge\n",
    "source_with_dates.createOrReplaceTempView(\"source_customer\")\n",
    "\n",
    "# Perform Merge Logic\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO incremental_load.default.dim_customer_v2 AS target\n",
    "USING source_customer AS source\n",
    "ON target.customer_id = source.customer_id AND target.end_date IS NULL\n",
    "\n",
    "-- When a match is found and data has changed, close the current record\n",
    "WHEN MATCHED AND (\n",
    "    target.name <> source.name OR\n",
    "    target.country <> source.country\n",
    ") THEN\n",
    "  UPDATE SET target.end_date = current_date()\n",
    "\n",
    "-- When no match is found, insert a new record\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (customer_id, name, country, start_date, end_date)\n",
    "  VALUES (source.customer_id, source.name, source.country, source.start_date, source.end_date)\n",
    "\"\"\")\n",
    "\n",
    "# Show the resulting dimension table to see the changes\n",
    "result_df = spark.sql(\"SELECT * FROM incremental_load.default.dim_customer_v2\")\n",
    "result_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2d81fac-2e05-43b7-aa29-491414985063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from incremental_load.default.dim_customer_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f2eb5f3-fd7a-4bc3-9367-1963ea367f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "bank Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d685cdef-129f-44e4-9366-ae664b8a026a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from datetime import date\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"SCD Type 2 Example\").getOrCreate()\n",
    "\n",
    "# Existing dim_bank_account (historical data)\n",
    "dim_data = [\n",
    "    (1001, 1, 1000, \"Active\", date(2020, 1, 1), None),  # Account 1001 is active since 2020-01-01\n",
    "    (1002, 2, 1500, \"Active\", date(2020, 1, 1), None),  # Account 1002 is active since 2020-01-01\n",
    "    (1003, 3, 500, \"Active\", date(2021, 1, 1), None),   # Account 1003 is active since 2021-01-01\n",
    "]\n",
    "dim_schema = StructType([\n",
    "    StructField(\"account_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"account_balance\", IntegerType(), False),\n",
    "    StructField(\"account_status\", StringType(), False),\n",
    "    StructField(\"start_date\", DateType(), False),\n",
    "    StructField(\"end_date\", DateType(), True),\n",
    "])\n",
    "\n",
    "dim_df = spark.createDataFrame(dim_data, schema=dim_schema)\n",
    "dim_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cbcd0b7-3f20-451b-865c-a4cf65dfc1ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Incoming data for source_bank_account\n",
    "source_data = [\n",
    "    (1001, 1, 1200, \"Active\"),  # Account 1001 balance updated\n",
    "    (1002, 2, 1500, \"Closed\"),  # Account 1002 status changed from Active to Closed\n",
    "    (1004, 4, 2000, \"Active\"),  # New account (1004)\n",
    "]\n",
    "source_schema = StructType([\n",
    "    StructField(\"account_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"account_balance\", IntegerType(), False),\n",
    "    StructField(\"account_status\", StringType(), False),\n",
    "])\n",
    "\n",
    "source_df = spark.createDataFrame(source_data, schema=source_schema)\n",
    "source_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6b26d1c-8f81-4642-92ab-18c1a3a9b553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from datetime import date\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"SCD Type 2 Example\").getOrCreate()\n",
    "\n",
    "# Existing dim_bank_account (historical data)\n",
    "dim_data = [\n",
    "    (1001, 1, 1000, \"Active\", date(2020, 1, 1), None),  # Account 1001 is active since 2020-01-01\n",
    "    (1002, 2, 1500, \"Active\", date(2020, 1, 1), None),  # Account 1002 is active since 2020-01-01\n",
    "    (1003, 3, 500, \"Active\", date(2021, 1, 1), None),   # Account 1003 is active since 2021-01-01\n",
    "]\n",
    "dim_schema = StructType([\n",
    "    StructField(\"account_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"account_balance\", IntegerType(), False),\n",
    "    StructField(\"account_status\", StringType(), False),\n",
    "    StructField(\"start_date\", DateType(), False),\n",
    "    StructField(\"end_date\", DateType(), True),\n",
    "])\n",
    "\n",
    "dim_df = spark.createDataFrame(dim_data, schema=dim_schema)\n",
    "dim_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"incremental_load.default.dim_bank_account\")\n",
    "# \n",
    "\n",
    "dim_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f12122e-33d1-45b8-8c2f-943b5639e217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Incoming data for source_bank_account\n",
    "source_data = [\n",
    "    (1004, 1, 3000, \"Active\"),  # Account 1001 balance updated\n",
    "    (1002, 2, 1500, \"Closed\"),  # Account 1002 status changed from Active to Closed\n",
    "    (1004, 4, 2000, \"Active\"),  # New account (1004)\n",
    "]\n",
    "source_schema = StructType([\n",
    "    StructField(\"account_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"account_balance\", IntegerType(), False),\n",
    "    StructField(\"account_status\", StringType(), False),\n",
    "])\n",
    "\n",
    "source_df = spark.createDataFrame(source_data, schema=source_schema)\n",
    "source_df.show()\n",
    "\n",
    "source_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"incremental_load.default.source_bank_account\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c62ea39-58f8-4bdc-8f2b-333767eb66af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_date, lit\n",
    "\n",
    "# Initialize Spark session (if not already initialized)\n",
    "spark = SparkSession.builder.appName(\"SCD_Type2_Example\").getOrCreate()\n",
    "\n",
    "# Step 1: Perform the MERGE operation for SCD Type 2 using `incremental_load.default` catalog and schema\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO incremental_load.default.dim_bank_account AS target\n",
    "USING incremental_load.default.source_bank_account AS source\n",
    "ON target.account_id = source.account_id AND target.end_date IS NULL\n",
    "\n",
    "-- When a match is found and data has changed, close the current record (end_date)\n",
    "WHEN MATCHED AND (\n",
    "    target.account_status <> source.account_status OR\n",
    "    target.account_balance <> source.account_balance\n",
    ") THEN\n",
    "  UPDATE SET \n",
    "  target.account_status = 'closed',\n",
    "  target.end_date = current_date()\n",
    "    -- Mark the current record as historical (end_date)\n",
    "\n",
    "-- When no match is found, insert a new record (new account or new entry)\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (account_id, customer_id, account_balance, account_status, start_date, end_date)\n",
    "  VALUES (source.account_id, source.customer_id, source.account_balance, source.account_status, current_date(), NULL)  -- New record is active with NULL end_date\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bfef967-974e-443e-ba92-aec0daf4a682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from incremental_load.default.dim_bank_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9a091bd-1b8f-4a5f-8454-ca99f553c20c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, current_date\n",
    "from datetime import date\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"bank_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"start_date\", DateType(), True),\n",
    "    StructField(\"end_date\", DateType(), True)  # Can be null\n",
    "])\n",
    "\n",
    "# Sample new data coming in\n",
    "new_data = spark.createDataFrame([\n",
    "    (1, \"ABC Bank\", \"New York\", date(2020, 1, 1), None),    # No end_date → active\n",
    "    (2, \"XYZ Bank\", \"Chicago\", date(2020, 5, 1), None),  # Has end_date → closed\n",
    "    (3, \"New Bank\", \"Houston\", date(2021, 6, 1), None)      # New bank, no history\n",
    "], schema) \\\n",
    "    .withColumn(\"status\", lit(\"active\"))  # Default, will be updated\n",
    "\n",
    "# Update status based on end_date\n",
    "new_data.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a12460-d0be-400a-aae9-43ca6cda47a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"bank_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"start_date\", StringType(), True),  # Temporarily as string\n",
    "    StructField(\"end_date\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data (dates still as strings here)\n",
    "data = [\n",
    "    (1, \"ABC Bank\", \"New York\", \"2020-01-01\", \"9999-12-31\", \"active\"),\n",
    "    (2, \"XYZ Bank\", \"Chicago\", \"2020-05-01\", \"9999-12-31\", \"active\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "history_data = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Convert date columns to proper DateType\n",
    "history_data = history_data.withColumn(\"start_date\", to_date(\"start_date\")) \\\n",
    "                           .withColumn(\"end_date\", to_date(\"end_date\"))\n",
    "history_data.display()\n",
    "\n",
    "# Save to Delta\n",
    "history_data.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"incremental_load.default.banks_history\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a65b716f-7df5-4c2a-8f05-2473b25d7197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from pyspark.sql.functions import to_date, lit, when\n",
    "updated_data = [\n",
    "    (1, \"ABC Bank\", \"Boston\", \"2024-01-01\", None),           # Changed location\n",
    "    (2, \"XYZ Bank\", \"Chicago\", \"2020-05-01\", \"2023-12-31\"),  # Closed bank\n",
    "    (3, \"New Bank\", \"Dallas\", \"2024-02-15\", None)            # New record\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "update_schema = StructType([\n",
    "    StructField(\"bank_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"start_date\", StringType(), True),\n",
    "    StructField(\"end_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df_updates = spark.createDataFrame(updated_data, schema=update_schema)\n",
    "\n",
    "# Convert dates to proper format\n",
    "df_updates = df_updates.withColumn(\"start_date\", to_date(\"start_date\")) \\\n",
    "                       .withColumn(\"end_date\", to_date(\"end_date\"))\n",
    "\n",
    "# Add derived status column based on end_date\n",
    "df_updates = df_updates.withColumn(\n",
    "    \"status\",\n",
    "    when(df_updates[\"end_date\"].isNotNull(), \"closed\").otherwise(\"active\")\n",
    ")\n",
    "\n",
    "df_updates.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b84e5764-3d2b-4cfd-a4bb-9bfd55c95f67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "# Load Delta Table\n",
    "delta_table = DeltaTable.forName(spark, \"incremental_load.default.banks_history\")\n",
    "\n",
    "# SCD Type 2 Merge Logic\n",
    "delta_table.alias(\"target\").merge(\n",
    "    df_updates.alias(\"source\"),\n",
    "    \"target.bank_id = source.bank_id AND target.end_date = '9999-12-31'\"\n",
    ").whenMatchedUpdate(condition=\"\"\"\n",
    "    target.name != source.name OR\n",
    "    target.location != source.location OR\n",
    "    target.status != source.status\n",
    "\"\"\", set={\n",
    "    \"end_date\": lit(date.today().isoformat()),  # Close the old record\n",
    "    \"status\": lit(\"closed\")\n",
    "}).whenNotMatchedInsert(values={\n",
    "    \"bank_id\": \"source.bank_id\",\n",
    "    \"name\": \"source.name\",\n",
    "    \"location\": \"source.location\",\n",
    "    \"start_date\": lit(date.today().isoformat()),\n",
    "    \"end_date\": lit(\"9999-12-31\"),\n",
    "    \"status\": \"source.status\"\n",
    "}).execute()\n",
    "\n",
    "# delta_table.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"incremental_load.default.banks_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d127263-35bf-4d07-b358-7827f0869c5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").table(\"incremental_load.default.banks_history\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6aa2373-fcd5-4c9c-9831-357a43afa2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table incremental_load.default.credit_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a5c1c81-195f-402d-a35b-cbda14084f17",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "initial_data"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, lit, expr\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import date\n",
    "\n",
    "delta_table_target = \"incremental_load.default.dim_customer4\"\n",
    "\n",
    "initial_data = [\n",
    "    (101, \"Alice\", \"New York\", date(2024, 1, 1), None,True),\n",
    "    (102, \"Bob\", \"San Francisco\", date(2024, 1, 1), None,True),\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Customer_ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"Start_Date\", DateType(), True),\n",
    "    StructField(\"End_Date\", DateType(), True),\n",
    "    StructField(\"IsCurrent\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "df_initial = spark.createDataFrame(initial_data, schema)\n",
    "\n",
    "df_initial.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta_table_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6befb09-6d8e-42b0-b8a4-1e9337e19587",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "new_data"
    }
   },
   "outputs": [],
   "source": [
    "incoming_data = [\n",
    "    (101, \"Alice\", \"New York\"),\n",
    "    (102, \"Bob\", \"Los Angeles\"),\n",
    "    (103, \"Charlie\", \"Chicago\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Customer_ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_incoming = spark.createDataFrame(incoming_data, schema)\n",
    "df_incoming = df_incoming.withColumn(\"Start_Date\", current_date()) \\\n",
    "        .withColumn(\"End_Date\", lit(\"9999-12-31\")) \\\n",
    "        .withColumn(\"IsCurrent\", lit(True))\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, delta_table_target)\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    source=df_incoming.alias(\"source\"),\n",
    "    condition=\"target.Customer_ID = source.Customer_ID AND target.IsCurrent = True\"\n",
    ").whenMatchedUpdate(\n",
    "    condition=\"\"\"\n",
    "        target.Name != source.Name OR  \n",
    "        target.City != source.City OR  \n",
    "    \"\"\",\n",
    "    set={\n",
    "        \"End_Date\": expr(\"current_date()\"),\n",
    "        \"IsCurrent\": lit(False)\n",
    "    }\n",
    "\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"Customer_ID\": \"source.Customer_ID\",\n",
    "        \"Name\": \"source.Name\",\n",
    "        \"City\": \"source.City\",\n",
    "        \"Start_Date\": expr(\"current_date()\"),\n",
    "        \"End_Date\": lit(\"9999-12-31\"),\n",
    "        \"IsCurrent\": lit(True)\n",
    "}).execute()\n",
    "\n",
    "\n",
    "spark.read.format(\"delta\").table(delta_table_target).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dd1bc45-7e24-4cbf-ae48-492605aab524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "delta_table_path = \"incremental_load.default.dim_credit_card_customers\"\n",
    "\n",
    "initial_data = [\n",
    "    (101, \"John Doe\", \"1234 Em st\", \"Platinum\", date(2021, 1, 1), None, True),\n",
    "    (102, \"Jane Smith\", \"4567 Oak st\", \"Gold\", date(2021, 1, 1), None, True)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Customer_ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"Card_Type\", StringType(), True),\n",
    "    StructField(\"Start_Date\", DateType(), True),\n",
    "    StructField(\"End_Date\", DateType(), True),\n",
    "    StructField(\"IsCurrent\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "df_initial = spark.createDataFrame(initial_data, schema)\n",
    "\n",
    "df_initial.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43a62bf3-10f1-4ab5-bd8d-47464917e281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "incoming_data = [\n",
    "    (101, \"John Doe\", \"1234 Main st\", \"Diamond\"),\n",
    "    (102, \"Jane Smith\", \"888* Maple Ave\", \"Gold\"),\n",
    "    (103, \"Alice Johnson\", \"9999 Pine st\", \"Platinum\")\n",
    "\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Customer_ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"Card_Type\", StringType(), True),\n",
    "])\n",
    "\n",
    "df_incoming = spark.createDataFrame(incoming_data, schema)\n",
    "\n",
    "df_incoming = df_incoming.withColumn(\"Start_Date\", current_date()) \\\n",
    "        .withColumn(\"End_Date\",lit(None))\\\n",
    "        .withColumn(\"IsCurrent\",lit(True))\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, delta_table_path)\n",
    "\n",
    "delta_table.alias(\"target\").merge(\n",
    "    source=df_incoming.alias(\"source\"),\n",
    "    condition=\"\"\"\n",
    "        target.Customer_ID = source.Customer_ID AND target.IsCurrent = True\n",
    "    \"\"\"\n",
    "\n",
    ").whenMatchedUpdate(\n",
    "    condition=\"\"\"\n",
    "        target.Name != source.Name OR  \n",
    "        target.Address != source.Address OR\n",
    "        target.Card_Type != source.Card_Type\n",
    "        \"\"\",\n",
    "    set = {\n",
    "        \"End_Date\": expr(\"current_date()\"),\n",
    "        \"IsCurrent\": lit(False)\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"Customer_ID\": \"source.Customer_ID\",\n",
    "        \"Name\": \"source.Name\",\n",
    "        \"Address\": \"source.Address\",\n",
    "        \"Card_Type\": \"source.Card_Type\",\n",
    "        \"Start_Date\": expr(\"current_date()\"),\n",
    "        \"End_Date\": lit(None),\n",
    "        \"IsCurrent\": lit(True)\n",
    "    }\n",
    ").execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb16a3ca-8880-49f9-a397-4186a18348fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").table(delta_table_path).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9be7f72c-26b4-46ec-afd3-1bb12f838ce6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6486286534895250,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Scd_type2_examples",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
