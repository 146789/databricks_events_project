{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c12f0d80-f6cf-4472-aaa7-86db15d8fd3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp, cast\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SCD_Type1_Delta\").getOrCreate()\n",
    "\n",
    "# Define explicit schema to avoid type inference issues\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"last_updated\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Initial records\n",
    "initial_data = [\n",
    "    (1, \"John Smith\", \"john.smith@example.com\", \"123-456-7890\"),\n",
    "    (2, \"Jane Doe\", \"jane.doe@example.com\", \"987-654-3210\"),\n",
    "    (3, \"Bob Wilson\", \"bob.w@example.com\", \"555-555-5555\")\n",
    "]\n",
    "\n",
    "# Create initial DataFrame with explicit schema\n",
    "initial_df = spark.createDataFrame(initial_data, [\"customer_id\", \"name\", \"email\", \"phone\"]) \\\n",
    "    .withColumn(\"last_updated\", current_timestamp()) \\\n",
    "    .select(\"customer_id\", \"name\", \"email\", \"phone\", \"last_updated\") \\\n",
    "    .cast(schema)\n",
    "print(\"initial_df\")\n",
    "display(initial_df)\n",
    "# Target Delta table path\n",
    "delta_table_path = \"incremental_load.default.customer_scd1\"\n",
    "\n",
    "# Create or overwrite Delta table with initial data\n",
    "try:\n",
    "    initial_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(delta_table_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Delta table: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "# Load Delta table\n",
    "try:\n",
    "    delta_table = DeltaTable.forName(spark, delta_table_path)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Delta table: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "# Display initial records\n",
    "print(\"Initial Records:\")\n",
    "spark.read.format(\"delta\").table(delta_table_path)\n",
    "\n",
    "# Update records (new/updated data)\n",
    "update_data = [\n",
    "    (1, \"John Doe\", \"john.doe@example.com\", \"123-456-7890\"),  # Updated name, email\n",
    "    (2, \"Jane Smith\", \"jane.smith@example.com\", \"987-654-3210\"),  # Updated name, email\n",
    "    (4, \"Alice Brown\", \"alice.b@example.com\", \"111-222-3333\")  # New record\n",
    "]\n",
    "\n",
    "# Create update DataFrame with same schema\n",
    "update_df = spark.createDataFrame(update_data, [\"customer_id\", \"name\", \"email\", \"phone\"]) \\\n",
    "    .withColumn(\"last_updated\", current_timestamp()) \\\n",
    "    .select(\"customer_id\", \"name\", \"email\", \"phone\", \"last_updated\")\n",
    "print(\"update_df\")\n",
    "display(update_df)\n",
    "\n",
    "# Perform SCD Type 1 merge\n",
    "try:\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        update_df.alias(\"source\"),\n",
    "        \"target.customer_id = source.customer_id\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"name\": \"source.name\",\n",
    "        \"email\": \"source.email\",\n",
    "        \"phone\": \"source.phone\",\n",
    "        \"last_updated\": \"source.last_updated\"\n",
    "    }).whenNotMatchedInsert(values={\n",
    "        \"customer_id\": \"source.customer_id\",\n",
    "        \"name\": \"source.name\",\n",
    "        \"email\": \"source.email\",\n",
    "        \"phone\": \"source.phone\",\n",
    "        \"last_updated\": \"source.last_updated\"\n",
    "    }).execute()\n",
    "except Exception as e:\n",
    "    print(f\"Error during merge operation: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "# Optimize Delta table\n",
    "try:\n",
    "    delta_table.vacuum()\n",
    "    delta_table.optimize().executeCompaction()\n",
    "except Exception as e:\n",
    "    print(f\"Error during table optimization: {str(e)}\")\n",
    "\n",
    "# Display final table\n",
    "print(\"Final Records after Update:\")\n",
    "final_df = spark.read.format(\"delta\").table(delta_table_path)\n",
    "final_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aadce5be-64a6-4211-8a36-3c0a6b4dd1b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SCD Type 1\").getOrCreate()\n",
    "\n",
    "# Simulate source and target\n",
    "df_source = spark.createDataFrame([\n",
    "    (1, \"John Doe\", \"john.doe@new.com\", \"123 Elm St\"),\n",
    "    (3, \"Bob Smith\", \"bob@example.com\", \"900 Pine Street\")\n",
    "], [\"customer_id\", \"name\", \"email\", \"address\"])\n",
    "print(\"source\")\n",
    "df_source.display()\n",
    "df_target = spark.createDataFrame([\n",
    "    (1, \"John Doe\", \"john@example.com\", \"123 Elm St\"),\n",
    "    (2, \"Alice Lee\", \"alice@old.com\", \"500 Maple Ave\")\n",
    "], [\"customer_id\", \"name\", \"email\", \"address\"])\n",
    "print(\"target\")\n",
    "df_target.display()\n",
    "# Step 1: Changed records (compare tracked columns)\n",
    "df_changed = df_source.alias(\"src\").join(df_target.alias(\"tgt\"), \"customer_id\") \\\n",
    "    .filter(\n",
    "        (col(\"src.name\") != col(\"tgt.name\")) |\n",
    "        (col(\"src.email\") != col(\"tgt.email\")) |\n",
    "        (col(\"src.address\") != col(\"tgt.address\"))\n",
    "    ).select(\"src.*\")\n",
    "print(\"changed\")\n",
    "df_changed.display()\n",
    "# Step 2: New records\n",
    "df_new = df_source.alias(\"src\").join(df_target.alias(\"tgt\"), \"customer_id\", \"left_anti\")\n",
    "print(\"new\")\n",
    "df_new.display()\n",
    "# Step 3: Union of new and changed records\n",
    "df_upserts = df_changed.union(df_new)\n",
    "print(\"df_upserts\")\n",
    "df_upserts.display()\n",
    "# Step 4: Remove old versions\n",
    "df_remaining = df_target.join(df_upserts, \"customer_id\", \"left_anti\")\n",
    "print(\"remaining\")\n",
    "df_remaining.display()\n",
    "# Step 5: Final dimension table\n",
    "df_final = df_remaining.union(df_upserts)\n",
    "print(\"final_df\")\n",
    "# Show result\n",
    "df_final.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3587ea49-7ebc-4036-8f66-12d8d7bad9e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "scd type1 example",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
